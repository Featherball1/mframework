{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "from mframework.data import ArrayDataset\n",
    "\n",
    "df = pd.read_parquet(\"data/mnist_train.parquet\")\n",
    "\n",
    "# Decode PNG bytes into numpy arrays\n",
    "images = []\n",
    "for raw in df[\"image\"]:\n",
    "    img = Image.open(io.BytesIO(raw[\"bytes\"]))\n",
    "    # Convert the images to grayscale and normalise\n",
    "    img = img.convert(\"L\")\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "     # Flatten the images into a vector\n",
    "    arr = arr.reshape(-1)\n",
    "    images.append(arr)\n",
    "\n",
    "X = np.stack(images)\n",
    "y = df[\"label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "# Wrap in ArrayDataset\n",
    "train_ds = ArrayDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72258a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet\n",
    "df = pd.read_parquet(\"data/mnist_test.parquet\")\n",
    "\n",
    "# Decode PNG bytes into numpy arrays\n",
    "images = []\n",
    "for raw in df[\"image\"]:\n",
    "    img = Image.open(io.BytesIO(raw[\"bytes\"]))\n",
    "    img = img.convert(\"L\")\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0 \n",
    "    arr = arr.reshape(-1)\n",
    "    images.append(arr)\n",
    "\n",
    "X = np.stack(images)\n",
    "y = df[\"label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "test_ds = ArrayDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mframework.autograd.tensor import Tensor\n",
    "from mframework.optim.sgd import SGD\n",
    "from mframework.nn import Linear, Sequential, CrossEntropyLoss, ReLU\n",
    "from mframework.data import DataLoader, SequentialSampler, BatchSampler\n",
    "from mframework.dtypes import DType\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Basic feedforward neural network model definition\n",
    "model = Sequential(\n",
    "    Linear(784, 128, True),\n",
    "    ReLU(),\n",
    "    Linear(128, 64, True),\n",
    "    ReLU(),\n",
    "    Linear(64, 10, True)\n",
    ")\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "optim = SGD(model.parameters(), lr)\n",
    "epochs = 20\n",
    "seq_sampler = SequentialSampler(train_ds)\n",
    "batch_sampler = BatchSampler(seq_sampler, batch_size=64, drop_last=False)\n",
    "train_dl = DataLoader(train_ds, batch_sampler)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i, batch in enumerate(tqdm(train_dl, desc=f\"epoch {epoch+1}\")):\n",
    "        # DataLoader yields (X_list, y_list) because of collate implementation\n",
    "        X_list, y_list = batch\n",
    "\n",
    "        # Stack / convert to numpy arrays\n",
    "        X_np = np.stack(X_list).astype(np.float32)\n",
    "        y_np = np.array(y_list, dtype=np.intp).reshape(-1)\n",
    "\n",
    "        # Wrap inputs/targets into Tensors\n",
    "        X_t = Tensor(X_np)\n",
    "        y_t = Tensor(y_np, dtype=DType.INT64)\n",
    "\n",
    "        # Run model forward pass and then update params\n",
    "        logits = model(X_t)\n",
    "        loss = criterion(logits, y_t)\n",
    "\n",
    "        epoch_loss += loss.item\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "    avg = epoch_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} avg loss: {avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ae2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_sampler_test = SequentialSampler(test_ds)\n",
    "batch_sampler_test = BatchSampler(seq_sampler_test, batch_size=64, drop_last=False)\n",
    "test_dl = DataLoader(test_ds, batch_sampler_test)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "for batch in test_dl:\n",
    "    X_list, y_list = batch\n",
    "    X_np = np.stack(X_list).astype(np.float32)\n",
    "    y_np = np.array(y_list, dtype=np.intp).reshape(-1)\n",
    "    X_t = Tensor(X_np)\n",
    "    y_t = Tensor(y_np, dtype=DType.INT64)\n",
    "    logits = model(X_t)\n",
    "    batch_loss = criterion(logits, y_t)\n",
    "    test_loss += float(batch_loss.data)\n",
    "    preds = np.argmax(logits.data, axis=1)\n",
    "    correct += np.sum(preds == y_np)\n",
    "    total += len(y_np)\n",
    "\n",
    "avg_loss = test_loss / len(test_dl)\n",
    "accuracy = correct / total\n",
    "print(f\"Test loss: {avg_loss:.4f}, Test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MariaPyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
